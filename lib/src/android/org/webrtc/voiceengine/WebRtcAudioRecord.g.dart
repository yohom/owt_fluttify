// ignore_for_file: non_constant_identifier_names, camel_case_types, missing_return, unused_import, unused_local_variable, dead_code, unnecessary_cast
//////////////////////////////////////////////////////////
// GENERATED BY FLUTTIFY. DO NOT EDIT IT.
//////////////////////////////////////////////////////////

import 'dart:typed_data';

import 'package:owt_fluttify/src/android/android.export.g.dart';
import 'package:flutter/foundation.dart';
import 'package:flutter/services.dart';

import 'package:foundation_fluttify/foundation_fluttify.dart';

class org_webrtc_voiceengine_WebRtcAudioRecord extends java_lang_Object  {
  //region constants
  static const String name__ = 'org.webrtc.voiceengine.WebRtcAudioRecord';

  
  //endregion

  //region creators
  
  //endregion

  //region getters
  
  //endregion

  //region setters
  
  //endregion

  //region methods
  
  static Future<void> setErrorCallback(org_webrtc_voiceengine_WebRtcAudioRecord_WebRtcAudioRecordErrorCallback errorCallback) async {
    // print log
    if (fluttifyLogEnabled) {
      debugPrint('fluttify-dart: org.webrtc.voiceengine.WebRtcAudioRecord::setErrorCallback([])');
    }
  
    // invoke native method
    final __result__ = await MethodChannel('com.fluttify/owt_fluttify').invokeMethod('org.webrtc.voiceengine.WebRtcAudioRecord::setErrorCallback', );
  
  
    // handle native call
    MethodChannel('org.webrtc.voiceengine.WebRtcAudioRecord::setErrorCallback::Callback')
        .setMethodCallHandler((methodCall) async {
          final args = methodCall.arguments as Map;
          switch (methodCall.method) {
            case 'Callback::org.webrtc.voiceengine.WebRtcAudioRecord.WebRtcAudioRecordErrorCallback::onWebRtcAudioRecordInitError':
              // print log
              if (fluttifyLogEnabled) {
                debugPrint('fluttify-dart-callback: onWebRtcAudioRecordInitError([\'var1\':${args['var1']}])');
              }
        
              // handle the native call
              errorCallback?.onWebRtcAudioRecordInitError(args['var1']);
              break;
            case 'Callback::org.webrtc.voiceengine.WebRtcAudioRecord.WebRtcAudioRecordErrorCallback::onWebRtcAudioRecordStartError':
              // print log
              if (fluttifyLogEnabled) {
                debugPrint('fluttify-dart-callback: onWebRtcAudioRecordStartError([\'var1\':${args['var1']}, \'var2\':${args['var2']}])');
              }
        
              // handle the native call
              errorCallback?.onWebRtcAudioRecordStartError((args['var1'] as int).toorg_webrtc_voiceengine_WebRtcAudioRecord_AudioRecordStartErrorCode(), args['var2']);
              break;
            case 'Callback::org.webrtc.voiceengine.WebRtcAudioRecord.WebRtcAudioRecordErrorCallback::onWebRtcAudioRecordError':
              // print log
              if (fluttifyLogEnabled) {
                debugPrint('fluttify-dart-callback: onWebRtcAudioRecordError([\'var1\':${args['var1']}])');
              }
        
              // handle the native call
              errorCallback?.onWebRtcAudioRecordError(args['var1']);
              break;
            default:
              break;
          }
        });
  
    // convert native result to dart side object
    if (__result__ == null) {
      return null;
    } else {
      final __return__ = __result__;
    
      return __return__;
    }
  }
  
  
  static Future<void> setOnAudioSamplesReady(org_webrtc_voiceengine_WebRtcAudioRecord_WebRtcAudioRecordSamplesReadyCallback callback) async {
    // print log
    if (fluttifyLogEnabled) {
      debugPrint('fluttify-dart: org.webrtc.voiceengine.WebRtcAudioRecord::setOnAudioSamplesReady([])');
    }
  
    // invoke native method
    final __result__ = await MethodChannel('com.fluttify/owt_fluttify').invokeMethod('org.webrtc.voiceengine.WebRtcAudioRecord::setOnAudioSamplesReady', );
  
  
    // handle native call
    MethodChannel('org.webrtc.voiceengine.WebRtcAudioRecord::setOnAudioSamplesReady::Callback')
        .setMethodCallHandler((methodCall) async {
          final args = methodCall.arguments as Map;
          switch (methodCall.method) {
        
            default:
              break;
          }
        });
  
    // convert native result to dart side object
    if (__result__ == null) {
      return null;
    } else {
      final __return__ = __result__;
    
      return __return__;
    }
  }
  
  
  static Future<void> setAudioSource(int source) async {
    // print log
    if (fluttifyLogEnabled) {
      debugPrint('fluttify-dart: org.webrtc.voiceengine.WebRtcAudioRecord::setAudioSource([\'source\':$source])');
    }
  
    // invoke native method
    final __result__ = await MethodChannel('com.fluttify/owt_fluttify').invokeMethod('org.webrtc.voiceengine.WebRtcAudioRecord::setAudioSource', {"source": source});
  
  
    // handle native call
  
  
    // convert native result to dart side object
    if (__result__ == null) {
      return null;
    } else {
      final __return__ = __result__;
    
      return __return__;
    }
  }
  
  
  static Future<void> setMicrophoneMute(bool mute) async {
    // print log
    if (fluttifyLogEnabled) {
      debugPrint('fluttify-dart: org.webrtc.voiceengine.WebRtcAudioRecord::setMicrophoneMute([\'mute\':$mute])');
    }
  
    // invoke native method
    final __result__ = await MethodChannel('com.fluttify/owt_fluttify').invokeMethod('org.webrtc.voiceengine.WebRtcAudioRecord::setMicrophoneMute', {"mute": mute});
  
  
    // handle native call
  
  
    // convert native result to dart side object
    if (__result__ == null) {
      return null;
    } else {
      final __return__ = __result__;
    
      return __return__;
    }
  }
  
  //endregion
}

extension org_webrtc_voiceengine_WebRtcAudioRecord_Batch on List<org_webrtc_voiceengine_WebRtcAudioRecord> {
  //region getters
  
  //endregion

  //region setters
  
  //endregion

  //region methods
  
  static Future<List<void>> setAudioSource_batch(List<int> source) async {
    if (false) {
      return Future.error('all args must have same length!');
    }
  
    // invoke native method
    final resultBatch = await MethodChannel('com.fluttify/owt_fluttify').invokeMethod('org.webrtc.voiceengine.WebRtcAudioRecord::setAudioSource_batch', [for (int __i__ = 0; __i__ < source.length; __i__++) {"source": source[__i__]}]);
  
  
    // convert native result to dart side object
    if (resultBatch == null) {
      return null;
    } else {
      final typedResult = (resultBatch as List).cast<String>().map((__result__) => __result__).toList();
    
      return typedResult;
    }
  }
  
  
  static Future<List<void>> setMicrophoneMute_batch(List<bool> mute) async {
    if (false) {
      return Future.error('all args must have same length!');
    }
  
    // invoke native method
    final resultBatch = await MethodChannel('com.fluttify/owt_fluttify').invokeMethod('org.webrtc.voiceengine.WebRtcAudioRecord::setMicrophoneMute_batch', [for (int __i__ = 0; __i__ < mute.length; __i__++) {"mute": mute[__i__]}]);
  
  
    // convert native result to dart side object
    if (resultBatch == null) {
      return null;
    } else {
      final typedResult = (resultBatch as List).cast<String>().map((__result__) => __result__).toList();
    
      return typedResult;
    }
  }
  
  //endregion
}